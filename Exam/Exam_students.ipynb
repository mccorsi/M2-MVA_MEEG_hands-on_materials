{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a11efca1-bebb-46f6-9d5e-f78dc28e7825",
   "metadata": {},
   "source": [
    "# M/EEG exam instructions\n",
    "- Assignment format: \n",
    "    - mandatory: a notebook with your answers \n",
    "    - optional: an additional document with your answers to the conceptual questions\n",
    "- Please send your assignment to M.Corsi's email address\n",
    "- Deadline: \n",
    "    - **March, 20th, 8AM.** Please note that an **extension will not be proposed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af3406f-1d6f-43d2-a6e0-2dfdf0b1b46b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T13:27:34.120395Z",
     "start_time": "2025-02-11T13:27:33.706243Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "import mne\n",
    "from mne.datasets import eegbci\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.stats import permutation_cluster_1samp_test as pcluster_test\n",
    "from mne.time_frequency import tfr_multitaper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0765cb29-56ce-4c5e-8533-4b27e3301938",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1 - Connectivity and Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be41298e-d10e-4a4b-830b-808eb4e3a74e",
   "metadata": {},
   "source": [
    "Here is an EEG dataset to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61d2193e-97a8-4161-8896-9f08b31944b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /Users/marieconstance.corsi/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R06.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Extracting EDF parameters from /Users/marieconstance.corsi/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R10.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Extracting EDF parameters from /Users/marieconstance.corsi/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R14.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Used Annotations descriptions: ['T1', 'T2']\n",
      "Not setting metadata\n",
      "45 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 45 events and 961 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#Define the parameters\n",
    "subject = 1  # use data from subject 1\n",
    "runs = [6, 10, 14]  # Motor imagery: hands vs feet\n",
    "\n",
    "# Extract raw data\n",
    "fnames = eegbci.load_data(subject=subject, runs=runs)\n",
    "raw = concatenate_raws([read_raw_edf(f, preload=True) for f in fnames])\n",
    "raw.rename_channels(lambda x: x.strip(\".\"))  # remove dots from channel names\n",
    "events, _ = mne.events_from_annotations(raw, event_id=dict(T1=2, T2=3))\n",
    "channel_names = raw.info['ch_names']\n",
    "\n",
    "# Extract trials between -1s and 4s\n",
    "tmin, tmax = -1, 4\n",
    "event_ids = dict(hands=2, feet=3)  # map event IDs to tasks\n",
    "epochs = mne.Epochs(\n",
    "    raw,\n",
    "    events,\n",
    "    event_ids,\n",
    "    tmin - 0.5,\n",
    "    tmax + 0.5,\n",
    "    baseline=None,\n",
    "    preload=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fe25e56-73e9-40f7-94bb-2e7760d57394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 3 columns\n",
      "Connectivity computation...\n",
      "only using indices for lower-triangular matrix\n",
      "    computing connectivity for 2016 connections\n",
      "    using t=-1.500s..4.500s for estimation (961 points)\n",
      "    frequencies: 8.2Hz..13.0Hz (30 points)\n",
      "    connectivity scores will be averaged for each band\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    the following metrics will be computed: Imaginary Coherence\n",
      "    computing cross-spectral density for epoch 1\n",
      "    computing cross-spectral density for epoch 2\n",
      "    computing cross-spectral density for epoch 3\n",
      "    computing cross-spectral density for epoch 4\n",
      "    computing cross-spectral density for epoch 5\n",
      "    computing cross-spectral density for epoch 6\n",
      "    computing cross-spectral density for epoch 7\n",
      "    computing cross-spectral density for epoch 8\n",
      "    computing cross-spectral density for epoch 9\n",
      "    computing cross-spectral density for epoch 10\n",
      "    computing cross-spectral density for epoch 11\n",
      "    computing cross-spectral density for epoch 12\n",
      "    computing cross-spectral density for epoch 13\n",
      "    computing cross-spectral density for epoch 14\n",
      "    computing cross-spectral density for epoch 15\n",
      "    computing cross-spectral density for epoch 16\n",
      "    computing cross-spectral density for epoch 17\n",
      "    computing cross-spectral density for epoch 18\n",
      "    computing cross-spectral density for epoch 19\n",
      "    computing cross-spectral density for epoch 20\n",
      "    computing cross-spectral density for epoch 21\n",
      "    assembling connectivity matrix\n",
      "[Connectivity computation done]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 14:29:26.633 python[19960:497918] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-11 14:29:26.633 python[19960:497918] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "from mne_connectivity import spectral_connectivity_epochs\n",
    "from mne_connectivity.viz import plot_connectivity_circle\n",
    "\n",
    "hands = epochs['hands']\n",
    "feet = epochs['feet']\n",
    "\n",
    "fmin, fmax = 8, 13 # mu band\n",
    "conhands = spectral_connectivity_epochs(hands, method='imcoh', fmin=fmin, fmax=fmax, faverage=True) # compute ImCoh of Mu-band averaged over epochs\n",
    "\n",
    "con_matrix_hands = conhands.get_data(\"dense\") # Get the data\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(figsize=(14, 5))\n",
    "im = axs.imshow(conhands.get_data(\"dense\"), vmin=0, vmax=1)\n",
    "fig.colorbar(im, ax=axs)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86733254-7769-47d0-a7cd-004b80ff9ed9",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "- For each condition:\n",
    "    - Compute and plot the connectivity matrices based on the estimation of the imaginary coherence averaged over the mu band and across the epochs. What does it say about potential changes between the tasks performed by the subject?\n",
    "    - Compute and plot the associated node strength averaged across the epochs. What does it say about potential changes between the tasks performed by the subject?\n",
    "\n",
    "- Here is the plot of the statistical difference between MI and Rest conditions obtained from imaginary coherence (left) and the results obtained with the node strength (right): What do you observe? Is it neurophysiologically meaningful?\n",
    "![Figure_ImCoh](./MI_Rest_ImCoh.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22149f58-5225-4608-831c-75a542d43db9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 2 - Features in BCI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee67d67-7f68-4277-a271-bb4b29fef966",
   "metadata": {},
   "source": [
    "Here is an EEG dataset to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2762da8a-dca3-4dcf-a5d6-626a414bed16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /Users/marieconstance.corsi/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R06.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Extracting EDF parameters from /Users/marieconstance.corsi/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R10.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Extracting EDF parameters from /Users/marieconstance.corsi/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R14.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Used Annotations descriptions: ['T1', 'T2']\n",
      "Not setting metadata\n",
      "45 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 45 events and 961 original time points ...\n",
      "0 bad epochs dropped\n",
      "NOTE: tfr_multitaper() is a legacy function. New code should use .compute_tfr(method=\"multitaper\").\n",
      "Applying baseline correction (mode: percent)\n",
      "Using a threshold of 1.724718\n",
      "stat_fun(H1): min=-4.1380237268582585 max=3.447987864084401\n",
      "Running initial clustering …\n",
      "Found 77 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8db928e45e8404e9bc590947a5fff1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 1 cluster to exclude from subsequent iterations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0f7f27867f4fdaae3e29f22a426602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #2 found 0 additional clusters to exclude from subsequent iterations\n",
      "Using a threshold of -1.724718\n",
      "stat_fun(H1): min=-4.1380237268582585 max=3.447987864084401\n",
      "Running initial clustering …\n",
      "Found 98 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4af4ba05fa45c58512d2b07853ab8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 0 clusters to exclude from subsequent iterations\n",
      "No baseline correction applied\n",
      "Using a threshold of 1.724718\n",
      "stat_fun(H1): min=-8.552076155477138 max=3.438132932036836\n",
      "Running initial clustering …\n",
      "Found 113 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6351bacb3d1e438893beb5e2b13119a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 0 clusters to exclude from subsequent iterations\n",
      "Using a threshold of -1.724718\n",
      "stat_fun(H1): min=-8.552076155477138 max=3.438132932036836\n",
      "Running initial clustering …\n",
      "Found 87 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398d97e9b677481d949b24e76af5f7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 1 cluster to exclude from subsequent iterations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add7e7cd06e145ed9635088fc6ab0a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #2 found 0 additional clusters to exclude from subsequent iterations\n",
      "No baseline correction applied\n",
      "Using a threshold of 1.724718\n",
      "stat_fun(H1): min=-6.323372484185545 max=5.715371704419454\n",
      "Running initial clustering …\n",
      "Found 149 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53b4598320747c5b45c4585433517d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 1 cluster to exclude from subsequent iterations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ef6f3bbb2248058777a60648b44955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #2 found 0 additional clusters to exclude from subsequent iterations\n",
      "Using a threshold of -1.724718\n",
      "stat_fun(H1): min=-6.323372484185545 max=5.715371704419454\n",
      "Running initial clustering …\n",
      "Found 77 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a505e51aa69040c5b0a3a1494993e876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 0 clusters to exclude from subsequent iterations\n",
      "No baseline correction applied\n",
      "Using a threshold of 1.713872\n",
      "stat_fun(H1): min=-5.6496212234254575 max=3.0699082407337825\n",
      "Running initial clustering …\n",
      "Found 63 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df76213d04904d33903cf563d1e50212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 0 clusters to exclude from subsequent iterations\n",
      "Using a threshold of -1.713872\n",
      "stat_fun(H1): min=-5.6496212234254575 max=3.0699082407337825\n",
      "Running initial clustering …\n",
      "Found 80 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f2aedbf018420e914db86a6008b364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 0 clusters to exclude from subsequent iterations\n",
      "No baseline correction applied\n",
      "Using a threshold of 1.713872\n",
      "stat_fun(H1): min=-3.754759498497272 max=3.3607039428313743\n",
      "Running initial clustering …\n",
      "Found 116 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65508ba920894a7fbb0ab430a01757dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 0 clusters to exclude from subsequent iterations\n",
      "Using a threshold of -1.713872\n",
      "stat_fun(H1): min=-3.754759498497272 max=3.3607039428313743\n",
      "Running initial clustering …\n",
      "Found 113 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c03eadbd9674a878336325d9e14d74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 0 clusters to exclude from subsequent iterations\n",
      "No baseline correction applied\n",
      "Using a threshold of 1.713872\n",
      "stat_fun(H1): min=-3.6160177594410863 max=3.5139046340567477\n",
      "Running initial clustering …\n",
      "Found 134 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bccfab76a24db99f3113014664b481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 1 cluster to exclude from subsequent iterations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3dee7efb764e69b411cb4d8e6ca9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #2 found 0 additional clusters to exclude from subsequent iterations\n",
      "Using a threshold of -1.713872\n",
      "stat_fun(H1): min=-3.6160177594410863 max=3.5139046340567477\n",
      "Running initial clustering …\n",
      "Found 60 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1093e4467940d4a1e0389e15de36bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/99 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 0 clusters to exclude from subsequent iterations\n",
      "No baseline correction applied\n"
     ]
    }
   ],
   "source": [
    "#Define the parameters\n",
    "subject = 1  # use data from subject 1\n",
    "runs = [6, 10, 14]  # Motor imagery: hands vs feet\n",
    "\n",
    "# Extract raw data\n",
    "fnames = eegbci.load_data(subject=subject, runs=runs)\n",
    "raw = concatenate_raws([read_raw_edf(f, preload=True) for f in fnames])\n",
    "raw.rename_channels(lambda x: x.strip(\".\"))  # remove dots from channel names\n",
    "events, _ = mne.events_from_annotations(raw, event_id=dict(T1=2, T2=3))\n",
    "\n",
    "# Extract trials between -1s and 4s\n",
    "channelsOfInterest = \"T7\", \"C3\", 'O1' # to get the full list of channels you can type: raw.info['ch_names']\n",
    "tmin, tmax = -1, 4\n",
    "event_ids = dict(hands=2, feet=3)  # map event IDs to tasks\n",
    "epochs = mne.Epochs(\n",
    "    raw,\n",
    "    events,\n",
    "    event_ids,\n",
    "    tmin - 0.5,\n",
    "    tmax + 0.5,\n",
    "    picks=(channelsOfInterest),\n",
    "    baseline=None,\n",
    "    preload=True,\n",
    ")\n",
    "\n",
    "# Compare power spectra computed in each condition/channel \n",
    "freqs = np.arange(2, 45)\n",
    "vmin, vmax = -1, 1.5  # set min and max ERDS values in plot\n",
    "baseline = (-1, 0)  # baseline interval (in s)\n",
    "cnorm = TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)  # min, center & max ERDS\n",
    "\n",
    "kwargs = dict(\n",
    "    n_permutations=100, step_down_p=0.05, seed=1, buffer_size=None, out_type=\"mask\"\n",
    ")  # for cluster test\n",
    "\n",
    "# Time-Frequency decomposition\n",
    "tfr = tfr_multitaper(\n",
    "    epochs,\n",
    "    freqs=freqs,\n",
    "    n_cycles=freqs,\n",
    "    use_fft=True,\n",
    "    return_itc=False,\n",
    "    average=False,\n",
    "    decim=2,\n",
    ")\n",
    "tfr.crop(tmin, tmax).apply_baseline(baseline, mode=\"percent\")\n",
    "\n",
    "nb_channels = len(channelsOfInterest)\n",
    "for event in event_ids:\n",
    "    # select desired epochs for visualization\n",
    "    tfr_ev = tfr[event]\n",
    "    fig, axes = plt.subplots(\n",
    "        1, 4, figsize=(12, 4), gridspec_kw={\"width_ratios\": [10, 10, 10, 1]}\n",
    "    )\n",
    "    for ch, ax in enumerate(axes[:-1]):  # for each channel\n",
    "        # positive clusters\n",
    "        _, c1, p1, _ = pcluster_test(tfr_ev.data[:, ch], tail=1, **kwargs)\n",
    "        # negative clusters\n",
    "        _, c2, p2, _ = pcluster_test(tfr_ev.data[:, ch], tail=-1, **kwargs)\n",
    "\n",
    "        # note that we keep clusters with p <= 0.05 from the combined clusters\n",
    "        # of two independent tests; in this example, we do not correct for\n",
    "        # these two comparisons\n",
    "        c = np.stack(c1 + c2, axis=2)  # combined clusters\n",
    "        p = np.concatenate((p1, p2))  # combined p-values\n",
    "        mask = c[..., p <= 0.05].any(axis=-1)\n",
    "\n",
    "        # plot TFR (ERDS map with masking)\n",
    "        tfr_ev.average().plot(\n",
    "            [ch],\n",
    "            cmap=\"RdBu\",\n",
    "            cnorm=cnorm,\n",
    "            axes=ax,\n",
    "            colorbar=False,\n",
    "            show=False,\n",
    "            mask=mask,\n",
    "            mask_style=\"mask\",\n",
    "        )\n",
    "\n",
    "        ax.set_title(epochs.ch_names[ch], fontsize=10)\n",
    "        ax.axvline(0, linewidth=1, color=\"black\", linestyle=\":\")  # event\n",
    "        if ch != 0:\n",
    "            ax.set_ylabel(\"\")\n",
    "            ax.set_yticklabels(\"\")\n",
    "    fig.colorbar(axes[0].images[-1], cax=axes[-1]).ax.set_yscale(\"linear\")\n",
    "    fig.suptitle(f\"ERDS ({event})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc159f64-b422-4c76-9ee1-562236006fcf",
   "metadata": {},
   "source": [
    "## Questions: \n",
    "- Please describe the observations you can make from the maps. Are there neurophysiologically relevant/meaningful?\n",
    "- To what extent such observations are informative of BCI performance?\n",
    "- If you were the experimenter, based on the previous observations, which (electrode(s); frequency bin(s)) couples would you pick to extract the features? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872d66e-039c-476d-b659-f176f3b3170f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3 - Machine Learning & BCI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c76632-92e0-4d9c-8d92-11f9744507c6",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "Here is a publicly available [BCI dataset](http://moabb.neurotechx.com/docs/generated/moabb.datasets.BNCI2014_001.html#moabb.datasets.BNCI2014_001) (cf below to load the data and to get information regarding the experimental information). In the following lines of code we defined two classification pipelines (CSP+LDA: Common Spatial Patterns + LDA, RG+LR:Riemannian Geometry + Logistic Regression), and we plotted their performances from a dataset composed of 2 subjects.\n",
    "\n",
    "## Questions\n",
    "- What observations can be made from the plots? \n",
    "- Instead of those implemented here, what framework would you propose to extract, select, and classify the features? Why? How would you assess the performance of your approach?\n",
    "\n",
    "## BONUS\n",
    "- Implement below your framework and compare it to the other pipelines (namely RG+LR and CSP+LDA). \n",
    "- What are your conclusions? Do you have some suggestion(s) to improve the performance of your framework?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9199147d-56ce-480e-a0df-47dd7d8dafa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 14:31:26,941 WARNING MainThread moabb.utils BNCI2014001 has been renamed to BNCI2014_001. BNCI2014001 will be removed in version 1.1.\n",
      "2025-02-11 14:31:26,941 WARNING MainThread moabb.datasets.base The dataset class name 'BNCI2014001' must be an abbreviation of its code 'BNCI2014-001'. See moabb.datasets.base.is_abbrev for more information.\n",
      "2025-02-11 14:31:26,942 INFO MainThread moabb.evaluations.base Processing dataset: BNCI2014-001\n",
      "BNCI2014-001-WithinSession:   0%|                                                                                                           | 0/2 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "from mne.decoding import CSP\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import moabb\n",
    "from moabb.datasets import BNCI2014001 # note: if you use MOABB>1.1, please change it as BNCI2014_001\n",
    "from moabb.evaluations import WithinSessionEvaluation\n",
    "from moabb.paradigms import LeftRightImagery\n",
    "\n",
    "moabb.set_log_level(\"info\")\n",
    "mne.set_log_level(\"CRITICAL\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "###### PIPELINES TO BE COMPARED (do not modify it!) ######\n",
    "# baseline pipeline to be used to make the comparison, please complete the following line with your framework\n",
    "pipelines = {}\n",
    "pipelines[\"CSP+LDA\"] = make_pipeline(CSP(n_components=8), LDA())\n",
    "pipelines[\"RG+LR\"] = make_pipeline(\n",
    "    Covariances(), TangentSpace(), LogisticRegression(solver=\"lbfgs\")\n",
    ")\n",
    "### BONUS - implementation of your framework ######\n",
    "#pipelines[\"MyPipeline\"] =\n",
    "\n",
    "\n",
    "###### DATASET TO BE USED (do not modify it!) - downloading it the first time can take some time ######\n",
    "dataset = BNCI2014001() # if you are using a more recent version of moabb please change it as BNCI2014_001\n",
    "subj = [1, 2]\n",
    "dataset.subject_list = subj\n",
    "\n",
    "\n",
    "###### DEFINITION OF THE PARADIGM & EVALUATION (do not modify it!) ######\n",
    "paradigm = LeftRightImagery()\n",
    "evaluation = WithinSessionEvaluation(\n",
    "    paradigm=paradigm, datasets=dataset, overwrite=False\n",
    ")\n",
    "results = evaluation.process(pipelines)\n",
    "#print(results.head()) # if you want to look at it...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "983471db-f9a2-49aa-9f7c-9012b0d0fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### SCRIPT TO PLOT THE RESULTS (do not modify it unless you want to add your framework!) #########\n",
    "\n",
    "# Plot the global distribution of the performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=[8, 4], sharey=True)\n",
    "\n",
    "sns.stripplot(\n",
    "    data=results,\n",
    "    y=\"score\",\n",
    "    x=\"pipeline\",\n",
    "    ax=axes[0],\n",
    "    jitter=True,\n",
    "    alpha=0.5,\n",
    "    zorder=1,\n",
    "    palette=\"rocket\",\n",
    ")\n",
    "sns.pointplot(data=results, y=\"score\", x=\"pipeline\", ax=axes[0], palette=\"rocket\")\n",
    "\n",
    "axes[0].set_ylabel(\"ROC AUC\")\n",
    "axes[0].set_ylim(0.5, 1)\n",
    "\n",
    "paired = results.pivot_table(\n",
    "    values=\"score\", columns=\"pipeline\", index=[\"subject\", \"session\"]\n",
    ")\n",
    "paired = paired.reset_index()\n",
    "\n",
    "sns.regplot(data=paired, y=\"RG+LR\", x=\"CSP+LDA\", ax=axes[1], fit_reg=False)\n",
    "axes[1].plot([0, 1], [0, 1], ls=\"--\", c=\"k\")\n",
    "axes[1].set_xlim(0.5, 1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot the individual distribution of the performance\n",
    "g = sns.catplot(\n",
    "    kind=\"bar\",\n",
    "    x=\"score\",\n",
    "    y=\"subject\",\n",
    "    hue=\"pipeline\",\n",
    "    col=\"dataset\",\n",
    "    height=12,\n",
    "    aspect=0.5,\n",
    "    data=results,\n",
    "    orient=\"h\",\n",
    "    palette=\"rocket\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155fb091-bb44-421d-8e8f-8fe9c6f2991c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 4 - Experimental considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e0058-7ee5-4c95-8fea-25f081e239c2",
   "metadata": {},
   "source": [
    "You plan to launch a new protocol based on EEG acquisitions:\n",
    "- What are the two main types of artifacts you may observe? Please indicate one example for each of them.\n",
    "- What are the main steps that compose an EEG processing pipeline?\n",
    "\n",
    "Now you are conducting and experimental protocol in BCI. It consists in 5 sessions of right hand motor imagery vs rest. After the fourth session training sessions subject Y still shows a global performance of 60%. \n",
    "At each session:\n",
    "- You instructed the subject to perform a right motor imagery when the visual target was up and to remain at rest when the visual target was down.\n",
    "- You always selected the same features (power spectra in CP3 at 10Hz & 14Hz, and in C3 at 12Hz & 16Hz).\n",
    "\n",
    "Based on these elements, what would be your suggestions to help the subject Y improving their performance at session 5?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
